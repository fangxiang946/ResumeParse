{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim as gs\n",
    "import jieba\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import gensim\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "pattern = re.compile(r'(\\d)')\n",
    "\n",
    "def clean_str(s):\n",
    "\ts = s.replace('？','?')\\\n",
    "\t\t.replace('。',' . ')\\\n",
    "\t\t.replace('，',',')\\\n",
    "\t\t.replace('；',' ; ')\\\n",
    "\t\t.replace('：',':')\\\n",
    "\t\t.replace('【','[')\\\n",
    "\t\t.replace('】',']')\\\n",
    "\t\t.replace('￥','$')\\\n",
    "\t\t.replace('……','^')\\\n",
    "\t\t.replace('、',',')\\\n",
    "\t\t.replace('‘',\"'\")\\\n",
    "\t\t.replace('’',\"'\")\\\n",
    "\t\t.replace('“','\"')\\\n",
    "\t\t.replace('”','\"')\\\n",
    "\t\t.replace('（','(')\\\n",
    "\t\t.replace('）',')')\n",
    "\ts = re.sub(r\"[^\\u4e00-\\u9fa5\\-\\.\\/\\@\\[A-Za-z0-9:(),!?\\'\\`]\", \" \", s)\n",
    "\ts = re.sub(r\" : \", \":\", s)\n",
    "\ts = re.sub(r\"\\'s\", \" \\'s\", s)\n",
    "\ts = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
    "\ts = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
    "\ts = re.sub(r\"\\'re\", \" \\'re\", s)\n",
    "\ts = re.sub(r\"\\'d\", \" \\'d\", s)\n",
    "\ts = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
    "\ts = re.sub(r\",\", \" , \", s)\n",
    "\ts = re.sub(r\"!\", \" ! \", s)\n",
    "\ts = re.sub(r\"\\(\", \" \\( \", s)\n",
    "\ts = re.sub(r\"\\)\", \" \\) \", s)\n",
    "\ts = re.sub(r\"\\[\", \" \\[ \", s)\n",
    "\ts = re.sub(r\"\\]\", \" \\] \", s)\n",
    "\ts = re.sub(r\"\\?\", \" \\? \", s)\n",
    "\ts = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "\twords=jieba.lcut(s.strip().lower(),HMM=False)\n",
    "\tresult=[]\n",
    "\tfor i in range(len(words)):\n",
    "\t\tword=words[i]\n",
    "\t\tlist=re.split(pattern,word)\n",
    "\t\tlist = [item for item in filter(lambda x:x != '', list)]\n",
    "\t\tresult=result+list\n",
    "\treturn result\n",
    "\n",
    "def pad_sentences(sentences,padding_word='<PAD/>',forced_sequence_length=None):\n",
    "\t\"\"\"pad sentences during training or prediction\"\"\"\n",
    "\tif forced_sequence_length is None:\n",
    "\t\tsequence_length=max(len(x) for x in sentences)\n",
    "\telse:\n",
    "\t\tlogging.critical('this is prediction ,readinig the trained sequence length')\n",
    "\t\tsequence_length=forced_sequence_length\n",
    "\tlogging.critical('the maximun length is {}'.format(sequence_length))\n",
    "\t\n",
    "\tpadded_sentences=[]\n",
    "\tfor i in range(len(sentences)):\n",
    "\t\tsentence=sentences[i]\n",
    "\t\tnum_padding=sequence_length-len(sentence)\n",
    "\t\t\n",
    "\t\tif num_padding<0:\n",
    "\t\t\tpadded_sentence=sentence[0:sequence_length]\n",
    "\t\t\tlogging.info('\"%s\" has to be cut off because it is longer than max_len '%(' '.join(padded_sentence)))\n",
    "\t\telse:\n",
    "\t\t\tpadded_sentence=sentence+[padding_word]*num_padding\n",
    "\t\tpadded_sentences.append(padded_sentence)\n",
    "\treturn padded_sentences\n",
    "\n",
    "def load_embeddings(vocabulary,word2vec_path=None):\n",
    "\tword_embeddings={}\n",
    "\tif word2vec_path is not None:\n",
    "\t\tword2vec = gensim.models.Word2Vec.load(word2vec_path)\n",
    "\tfor word in vocabulary:\n",
    "\t\tif word2vec_path is not None and word in word2vec.wv.vocab:\n",
    "\t\t\tword_embeddings[word]=word2vec.wv[word]\n",
    "\t\telse:\n",
    "\t\t\tword_embeddings[word] = np.random.uniform(-0.25, 0.25, 256)\n",
    "\tdel word2vec\n",
    "\treturn word_embeddings\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "\tdata = np.array(data)\n",
    "\tdata_size = len(data)\n",
    "\tnum_batches_per_epoch = int(data_size / batch_size) + 1\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tif shuffle:\n",
    "\t\t\tshuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "\t\t\tshuffled_data = data[shuffle_indices]\n",
    "\t\telse:\n",
    "\t\t\tshuffled_data = data\n",
    "\n",
    "\t\tfor batch_num in range(num_batches_per_epoch):\n",
    "\t\t\tstart_index = batch_num * batch_size\n",
    "\t\t\tend_index = min((batch_num + 1) * batch_size, data_size)\n",
    "\t\t\tyield shuffled_data[start_index:end_index]\n",
    "\n",
    "\n",
    "def bulid_vocab(sentences):\n",
    "\tword_counts=Counter(itertools.chain(*sentences))\n",
    "\tvocabulary_inv=[word[0] for word in word_counts.most_common()]#按词频构造字典\n",
    "\tvocabulary={word:index for index,word in enumerate(vocabulary_inv)}\n",
    "\treturn vocabulary,vocabulary_inv\n",
    "\n",
    "def load_data(filename,cnum=100):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df[:cnum] \n",
    "    selected=['Category','Text']\n",
    "    non_selected=list(set(df.columns)-set(selected))\n",
    "\n",
    "    df=df.drop(non_selected,axis=1)#去掉不需要的列\n",
    "    df=df.dropna(axis=0,how='any',subset=selected)#去掉空行\n",
    "    df=df.reindex(np.random.permutation(df.index))#打乱行顺序\n",
    "\n",
    "    labels=sorted(list(set(df[selected[0]].tolist())))#分类标签\n",
    "    num_labels=len(labels)\n",
    "    one_hot=np.zeros((num_labels,num_labels),int)\n",
    "    np.fill_diagonal(one_hot,1)\n",
    "    label_dict=dict(zip(labels,one_hot))\n",
    "\n",
    "    x_raw=df[selected[1]].apply(lambda x:clean_str(x)).tolist()\n",
    "    y_raw=df[selected[0]].apply(lambda y:label_dict[y]).tolist()\n",
    "\n",
    "    x_raw=pad_sentences(x_raw)\n",
    "    vocabulary,vocabulary_inv=bulid_vocab(x_raw)\n",
    "\n",
    "    x=np.array([[vocabulary[word] for word in sentence] for sentence in x_raw])\n",
    "    y=np.array(y_raw)\n",
    "\n",
    "    return x,y,vocabulary,vocabulary_inv,df,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    input_file = '../data/train.csv'\n",
    "    x_, y_, vocabulary, vocabulary_inv, df, labels = load_data(input_file,cnum=8000)\n",
    "\n",
    "    training_config = '../training_config.json'\n",
    "    params = json.loads(open(training_config, encoding='utf-8').read())\n",
    "\n",
    "    # 给每个单词分配一个256维度的向量\n",
    "    word_embeddings = load_embeddings(vocabulary, params['word2vec_path'])\n",
    "    # 构造输入矩阵\n",
    "    embedding_mat = [word_embeddings[word] for index, word in enumerate(vocabulary_inv)]\n",
    "    embedding_mat = np.array(embedding_mat, dtype=np.float32)\n",
    "\n",
    "    # 将原始数据分割为训练数据和测试数据\n",
    "    x, x_test, y, y_test = train_test_split(x_, y_, test_size=0.2)\n",
    "\n",
    "    # 将训练数据分割为训练数据和验证数据\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    logging.info('x_train:{},x_val:{},x_test:{}'.format(len(x_train), len(x_val), len(x_test)))\n",
    "    logging.info('y_train:{},y_val:{},y_test:{}'.format(len(y_train), len(y_val), len(y_test)))\n",
    "\n",
    "    return x_train,y_train,x_val,y_val,embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\FANGXI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.730 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "CRITICAL:root:the maximun length is 45\n",
      "INFO:gensim.utils:loading Word2Vec object from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx\n",
      "INFO:gensim.utils:loading wv recursively from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading syn0 from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.wv.syn0.npy with mmap=None\n",
      "INFO:gensim.utils:loading syn1neg from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.syn1neg.npy with mmap=None\n",
      "INFO:gensim.models.word2vec:Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.\n",
      "INFO:gensim.models.deprecated.old_saveload:loading Word2Vec object from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx\n",
      "INFO:gensim.models.deprecated.old_saveload:loading wv recursively from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading syn0 from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.wv.syn0.npy with mmap=None\n",
      "INFO:gensim.models.deprecated.old_saveload:loading syn1neg from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.syn1neg.npy with mmap=None\n",
      "INFO:gensim.models.deprecated.old_saveload:loading syn1 from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.syn1.npy with mmap=None\n",
      "INFO:gensim.models.deprecated.old_saveload:setting ignored attribute syn0norm to None\n",
      "INFO:gensim.models.deprecated.old_saveload:setting ignored attribute cum_table to None\n",
      "INFO:gensim.models.deprecated.old_saveload:loaded D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx\n",
      "INFO:root:x_train:5120,x_val:1280,x_test:1600\n",
      "INFO:root:y_train:5120,y_val:1280,y_test:1600\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_val,y_val,embedding_mat = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,LSTM,Embedding,recurrent,Bidirectional,Flatten,Dropout\n",
    "from keras.models import Model,Sequential\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "Epoch=1\n",
    "BATCH_SIZE=100\n",
    "BiRNN_UNITS=200\n",
    "def train_crf_model(x_train, y_train,x_val, y_val, embedding_mat):\n",
    "    inputSize,inputLength = x_train.shape\n",
    "    print('inputLength=%s'%inputLength)\n",
    "    inputDim,outputDim = embedding_mat.shape\n",
    "    print('inputDim=%s,outputDim=%s'%(inputDim,outputDim))\n",
    "    y_length,class_num = y_train.shape\n",
    "    print('class_num=%s'%class_num)\n",
    "    \n",
    "    # define model    \n",
    "    \n",
    "    #model = Sequential()\n",
    "    #model.add(Embedding(inputDim,outputDim,weights=[embedding_mat], trainable=False,input_length=inputLength))  # Random embedding mask_zero=True\n",
    "    #model.add(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True)))\n",
    "    #crf = CRF(class_num, sparse_target=True)\n",
    "    #model.add(crf)\n",
    "    #model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "    \n",
    "    inputs = Input(shape=(inputLength,))\n",
    "    x = Embedding(inputDim,outputDim,weights=[embedding_mat], trainable=False,mask_zero=True)(inputs)\n",
    "    x = LSTM(outputDim)(x)\n",
    "    x = CRF(10)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(class_num, activation='softmax')(x)\n",
    "    \n",
    "    model=Model(inputs,preds)\n",
    "     # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # summarize the model\n",
    "    print(model.summary())      \n",
    "    model.fit(x_train,y_train,epochs=Epoch,batch_size=BATCH_SIZE) #,validation_data=(x_val,y_val)\n",
    "    model.save('../ckpt/crf.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, 45)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, 34)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputLength=45\n",
      "inputDim=7677,outputDim=256\n",
      "class_num=34\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Index out of range using input dim 2; input has only 2 dims for 'crf_5/strided_slice' (op: 'StridedSlice') with input shapes: [?,10], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    687\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Index out of range using input dim 2; input has only 2 dims for 'crf_5/strided_slice' (op: 'StridedSlice') with input shapes: [?,10], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-5641f4ba00b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcrf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_crf_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-6731b135ce42>\u001b[0m in \u001b[0;36mtrain_crf_model\u001b[1;34m(x_train, y_train, x_val, y_val, embedding_mat)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputDim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_mat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask_zero\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputDim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCRF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\我要回珠海\\实战项目\\ResumeParse\\keras_contrib\\layers\\crf.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, X, mask)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'viterbi'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             \u001b[0mtest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviterbi_decoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[0mtest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_marginal_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\我要回珠海\\实战项目\\ResumeParse\\keras_contrib\\layers\\crf.py\u001b[0m in \u001b[0;36mviterbi_decoding\u001b[1;34m(self, X, mask)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_boundary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             input_energy = self.add_boundary_energy(\n\u001b[1;32m--> 562\u001b[1;33m                 input_energy, mask, self.left_boundary, self.right_boundary)\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0margmin_tables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_energy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_logZ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\我要回珠海\\实战项目\\ResumeParse\\keras_contrib\\layers\\crf.py\u001b[0m in \u001b[0;36madd_boundary_energy\u001b[1;34m(self, energy, mask, start, end)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             energy = K.concatenate([energy[:, :1, :] + start, energy[:, 1:, :]],\n\u001b[0m\u001b[0;32m    397\u001b[0m                                    axis=1)\n\u001b[0;32m    398\u001b[0m             energy = K.concatenate([energy[:, :-1, :], energy[:, -1:, :] + end],\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    735\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m   7405\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7406\u001b[0m         \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshrink_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshrink_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7407\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   7408\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7409\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   3160\u001b[0m         op_def=op_def)\n\u001b[0;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[1;32m-> 3162\u001b[1;33m                            compute_device=compute_device)\n\u001b[0m\u001b[0;32m   3163\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[1;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   3206\u001b[0m     \u001b[1;31m# compute_shapes argument.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3208\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3209\u001b[0m     \u001b[1;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2425\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2426\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2427\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2400\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2401\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2329\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2330\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python环境\\learn\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Index out of range using input dim 2; input has only 2 dims for 'crf_5/strided_slice' (op: 'StridedSlice') with input shapes: [?,10], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>."
     ]
    }
   ],
   "source": [
    "crf_model = train_crf_model(x_train,y_train,x_val, y_val,embedding_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 45)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, 34)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, 45)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7677, 256)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense, Dropout,Flatten,Conv1D,MaxPooling1D\n",
    "\n",
    "def train_cnn_model(x_train, y_train,x_val, y_val, embedding_mat):\n",
    "    inputSize,inputLength = x_train.shape\n",
    "    print('inputLength=%s'%inputLength)\n",
    "    inputDim,outputDim = embedding_mat.shape\n",
    "    print('inputDim=%s,outputDim=%s'%(inputDim,outputDim))\n",
    "    _,class_num = y_train.shape\n",
    "    print('class_num=%s'%class_num)\n",
    "    \n",
    "    inputs = Input(shape=(inputLength,))\n",
    "    x = Embedding(inputDim,outputDim,weights=[embedding_mat], trainable=False)(inputs)\n",
    "    \n",
    "    x = Conv1D(32,5,activation='relu')(x)\n",
    "    x = MaxPooling1D()(x)\n",
    "\n",
    "    x = Conv1D(64,5,activation='relu')(x)\n",
    "    x = MaxPooling1D()(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(class_num, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs,outputs)\n",
    "    # 损失函数使用交叉熵\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(x_train,y_train,epochs=Epoch,batch_size=BATCH_SIZE,validation_data=(x_val,y_val))\n",
    "    model.save('../ckpt/cnn.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputLength=45\n",
      "inputDim=7677,outputDim=256\n",
      "class_num=34\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 45, 256)           1965312   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 41, 32)            40992     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 16, 64)            10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 34)                4386      \n",
      "=================================================================\n",
      "Total params: 2,086,658\n",
      "Trainable params: 121,346\n",
      "Non-trainable params: 1,965,312\n",
      "_________________________________________________________________\n",
      "Train on 5120 samples, validate on 1280 samples\n",
      "Epoch 1/1\n",
      "5120/5120 [==============================] - 3s 542us/step - loss: 1.9430 - acc: 0.4613 - val_loss: 1.7095 - val_acc: 0.5492\n"
     ]
    }
   ],
   "source": [
    "cn_model = train_cnn_model(x_train,y_train,x_val, y_val,embedding_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense, Dropout,Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "def train_cnn_model(x_train, y_train,x_val, y_val, embedding_mat):\n",
    "    \n",
    "    x_train = [[embedding_mat[w] for w in s] for s in x_train]\n",
    "    x_train = np.array(x_train, dtype=np.float32)\n",
    "    \n",
    "    _,vocab_size,word2vec_size = x_train.shape\n",
    "    x_train = x_train.reshape(-1,vocab_size,word2vec_size,1)\n",
    "    _,class_num = y_train.shape\n",
    "    \n",
    "    x_val = [[embedding_mat[w] for w in s] for s in x_val]\n",
    "    x_val = np.array(x_val, dtype=np.float32)\n",
    "    x_val = x_val.reshape(-1, vocab_size, word2vec_size, 1)\n",
    "    \n",
    "    inputs = Input(shape=(vocab_size,word2vec_size,1))   #width,height,channels\n",
    "\n",
    "    x = Conv2D(32,(5,5),activation='relu')(inputs)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(64,(5,5),activation='relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(class_num, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs,outputs)\n",
    "    # 损失函数使用交叉熵\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(x_train,y_train,epochs=Epoch,batch_size=BATCH_SIZE,validation_data=(x_val,y_val))\n",
    "    model.save('../ckpt/cnn.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
