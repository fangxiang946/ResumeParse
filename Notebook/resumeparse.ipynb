{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim as gs\n",
    "import jieba\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import gensim\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "pattern = re.compile(r'(\\d)')\n",
    "\n",
    "def clean_str(s):\n",
    "\ts = s.replace('？','?')\\\n",
    "\t\t.replace('。',' . ')\\\n",
    "\t\t.replace('，',',')\\\n",
    "\t\t.replace('；',' ; ')\\\n",
    "\t\t.replace('：',':')\\\n",
    "\t\t.replace('【','[')\\\n",
    "\t\t.replace('】',']')\\\n",
    "\t\t.replace('￥','$')\\\n",
    "\t\t.replace('……','^')\\\n",
    "\t\t.replace('、',',')\\\n",
    "\t\t.replace('‘',\"'\")\\\n",
    "\t\t.replace('’',\"'\")\\\n",
    "\t\t.replace('“','\"')\\\n",
    "\t\t.replace('”','\"')\\\n",
    "\t\t.replace('（','(')\\\n",
    "\t\t.replace('）',')')\n",
    "\ts = re.sub(r\"[^\\u4e00-\\u9fa5\\-\\.\\/\\@\\[A-Za-z0-9:(),!?\\'\\`]\", \" \", s)\n",
    "\ts = re.sub(r\" : \", \":\", s)\n",
    "\ts = re.sub(r\"\\'s\", \" \\'s\", s)\n",
    "\ts = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
    "\ts = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
    "\ts = re.sub(r\"\\'re\", \" \\'re\", s)\n",
    "\ts = re.sub(r\"\\'d\", \" \\'d\", s)\n",
    "\ts = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
    "\ts = re.sub(r\",\", \" , \", s)\n",
    "\ts = re.sub(r\"!\", \" ! \", s)\n",
    "\ts = re.sub(r\"\\(\", \" \\( \", s)\n",
    "\ts = re.sub(r\"\\)\", \" \\) \", s)\n",
    "\ts = re.sub(r\"\\[\", \" \\[ \", s)\n",
    "\ts = re.sub(r\"\\]\", \" \\] \", s)\n",
    "\ts = re.sub(r\"\\?\", \" \\? \", s)\n",
    "\ts = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "\twords=jieba.lcut(s.strip().lower(),HMM=False)\n",
    "\tresult=[]\n",
    "\tfor i in range(len(words)):\n",
    "\t\tword=words[i]\n",
    "\t\tlist=re.split(pattern,word)\n",
    "\t\tlist = [item for item in filter(lambda x:x != '', list)]\n",
    "\t\tresult=result+list\n",
    "\treturn result\n",
    "\n",
    "def pad_sentences(sentences,padding_word='<PAD/>',forced_sequence_length=None):\n",
    "\t\"\"\"pad sentences during training or prediction\"\"\"\n",
    "\tif forced_sequence_length is None:\n",
    "\t\tsequence_length=max(len(x) for x in sentences)\n",
    "\telse:\n",
    "\t\tlogging.critical('this is prediction ,readinig the trained sequence length')\n",
    "\t\tsequence_length=forced_sequence_length\n",
    "\tlogging.critical('the maximun length is {}'.format(sequence_length))\n",
    "\t\n",
    "\tpadded_sentences=[]\n",
    "\tfor i in range(len(sentences)):\n",
    "\t\tsentence=sentences[i]\n",
    "\t\tnum_padding=sequence_length-len(sentence)\n",
    "\t\t\n",
    "\t\tif num_padding<0:\n",
    "\t\t\tpadded_sentence=sentence[0:sequence_length]\n",
    "\t\t\tlogging.info('\"%s\" has to be cut off because it is longer than max_len '%(' '.join(padded_sentence)))\n",
    "\t\telse:\n",
    "\t\t\tpadded_sentence=sentence+[padding_word]*num_padding\n",
    "\t\tpadded_sentences.append(padded_sentence)\n",
    "\treturn padded_sentences\n",
    "\n",
    "def load_embeddings(vocabulary,word2vec_path=None):\n",
    "\tword_embeddings={}\n",
    "\tif word2vec_path is not None:\n",
    "\t\tword2vec = gensim.models.Word2Vec.load(word2vec_path)\n",
    "\tfor word in vocabulary:\n",
    "\t\tif word2vec_path is not None and word in word2vec.wv.vocab:\n",
    "\t\t\tword_embeddings[word]=word2vec.wv[word]\n",
    "\t\telse:\n",
    "\t\t\tword_embeddings[word] = np.random.uniform(-0.25, 0.25, 256)\n",
    "\tdel word2vec\n",
    "\treturn word_embeddings\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "\tdata = np.array(data)\n",
    "\tdata_size = len(data)\n",
    "\tnum_batches_per_epoch = int(data_size / batch_size) + 1\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tif shuffle:\n",
    "\t\t\tshuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "\t\t\tshuffled_data = data[shuffle_indices]\n",
    "\t\telse:\n",
    "\t\t\tshuffled_data = data\n",
    "\n",
    "\t\tfor batch_num in range(num_batches_per_epoch):\n",
    "\t\t\tstart_index = batch_num * batch_size\n",
    "\t\t\tend_index = min((batch_num + 1) * batch_size, data_size)\n",
    "\t\t\tyield shuffled_data[start_index:end_index]\n",
    "\n",
    "\n",
    "def bulid_vocab(sentences):\n",
    "\tword_counts=Counter(itertools.chain(*sentences))\n",
    "\tvocabulary_inv=[word[0] for word in word_counts.most_common()]#按词频构造字典\n",
    "\tvocabulary={word:index for index,word in enumerate(vocabulary_inv)}\n",
    "\treturn vocabulary,vocabulary_inv\n",
    "\n",
    "def load_data(filename,cnum=100):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df[:cnum] \n",
    "    selected=['Category','Text']\n",
    "    non_selected=list(set(df.columns)-set(selected))\n",
    "\n",
    "    df=df.drop(non_selected,axis=1)#去掉不需要的列\n",
    "    df=df.dropna(axis=0,how='any',subset=selected)#去掉空行\n",
    "    df=df.reindex(np.random.permutation(df.index))#打乱行顺序\n",
    "\n",
    "    labels=sorted(list(set(df[selected[0]].tolist())))#分类标签\n",
    "    num_labels=len(labels)\n",
    "    one_hot=np.zeros((num_labels,num_labels),int)\n",
    "    np.fill_diagonal(one_hot,1)\n",
    "    label_dict=dict(zip(labels,one_hot))\n",
    "\n",
    "    x_raw=df[selected[1]].apply(lambda x:clean_str(x)).tolist()\n",
    "    y_raw=df[selected[0]].apply(lambda y:label_dict[y]).tolist()\n",
    "\n",
    "    x_raw=pad_sentences(x_raw)\n",
    "    vocabulary,vocabulary_inv=bulid_vocab(x_raw)\n",
    "\n",
    "    x=np.array([[vocabulary[word] for word in sentence] for sentence in x_raw])\n",
    "    y=np.array(y_raw)\n",
    "\n",
    "    return x,y,vocabulary,vocabulary_inv,df,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    input_file = '../data/train.csv'\n",
    "    x_, y_, vocabulary, vocabulary_inv, df, labels = load_data(input_file,cnum=8000)\n",
    "\n",
    "    training_config = '../training_config.json'\n",
    "    params = json.loads(open(training_config, encoding='utf-8').read())\n",
    "\n",
    "    # 给每个单词分配一个256维度的向量\n",
    "    word_embeddings = load_embeddings(vocabulary, params['word2vec_path'])\n",
    "    # 构造输入矩阵\n",
    "    embedding_mat = [word_embeddings[word] for index, word in enumerate(vocabulary_inv)]\n",
    "    embedding_mat = np.array(embedding_mat, dtype=np.float32)\n",
    "\n",
    "    # 将原始数据分割为训练数据和测试数据\n",
    "    x, x_test, y, y_test = train_test_split(x_, y_, test_size=0.2)\n",
    "\n",
    "    # 将训练数据分割为训练数据和验证数据\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    logging.info('x_train:{},x_val:{},x_test:{}'.format(len(x_train), len(x_val), len(x_test)))\n",
    "    logging.info('y_train:{},y_val:{},y_test:{}'.format(len(y_train), len(y_val), len(y_test)))\n",
    "\n",
    "    return x_train,y_train,x_val,y_val,embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:the maximun length is 45\n",
      "INFO:gensim.utils:loading Word2Vec object from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx\n",
      "INFO:gensim.utils:loading wv recursively from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading syn0 from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.wv.syn0.npy with mmap=None\n",
      "INFO:gensim.utils:loading syn1neg from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.syn1neg.npy with mmap=None\n",
      "INFO:gensim.models.word2vec:Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.\n",
      "INFO:gensim.models.deprecated.old_saveload:loading Word2Vec object from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx\n",
      "INFO:gensim.models.deprecated.old_saveload:loading wv recursively from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading syn0 from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.wv.syn0.npy with mmap=None\n",
      "INFO:gensim.models.deprecated.old_saveload:loading syn1neg from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.syn1neg.npy with mmap=None\n",
      "INFO:gensim.models.deprecated.old_saveload:loading syn1 from D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx.syn1.npy with mmap=None\n",
      "INFO:gensim.models.deprecated.old_saveload:setting ignored attribute syn0norm to None\n",
      "INFO:gensim.models.deprecated.old_saveload:setting ignored attribute cum_table to None\n",
      "INFO:gensim.models.deprecated.old_saveload:loaded D:/我要回珠海/实战项目/MyDataSets/word2vec_from_weixin/word2vec/word2vec_wx\n",
      "INFO:root:x_train:5120,x_val:1280,x_test:1600\n",
      "INFO:root:y_train:5120,y_val:1280,y_test:1600\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_val,y_val,embedding_mat = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding, recurrent, Bidirectional, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "Epoch = 1\n",
    "BATCH_SIZE = 100\n",
    "BiRNN_UNITS = 100\n",
    "\n",
    "\n",
    "def get_model(x_train, y_train, embedding_mat):\n",
    "    inputSize, inputLength = x_train.shape\n",
    "    #print('inputLength=%s' % inputLength)\n",
    "    inputDim, outputDim = embedding_mat.shape\n",
    "    #print('inputDim=%s,outputDim=%s' % (inputDim, outputDim))\n",
    "    _, class_num = y_train.shape\n",
    "    #print('class_num=%s' % class_num)\n",
    "\n",
    "    #with open('../ckpt/embedding_mat.pkl', 'wb') as outp:\n",
    "    #    pickle.dump((inputLength, class_num, embedding_mat), outp)\n",
    "\n",
    "    # define model\n",
    "    inputs = Input(shape=(inputLength,))\n",
    "    x = Embedding(inputDim, outputDim,embeddings_initializer=Constant(embedding_mat), trainable=False)(inputs)  # mask_zero=True\n",
    "    x = Bidirectional(LSTM(128))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    preds = Dense(class_num, activation='softmax')(x)\n",
    "    model = Model(inputs,preds)\n",
    "    model.compile('adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7677, 256)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 45, 256)           1965312   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 34)                8738      \n",
      "=================================================================\n",
      "Total params: 2,368,290\n",
      "Trainable params: 402,978\n",
      "Non-trainable params: 1,965,312\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "crf_model = get_model(x_train,y_train,embedding_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5120 samples, validate on 1280 samples\n",
      "Epoch 1/3\n",
      "5120/5120 [==============================] - 12s 2ms/step - loss: 2.4543 - acc: 0.3855 - val_loss: 1.8687 - val_acc: 0.4828\n",
      "Epoch 2/3\n",
      "5120/5120 [==============================] - 10s 2ms/step - loss: 1.7234 - acc: 0.5311 - val_loss: 1.5026 - val_acc: 0.6047\n",
      "Epoch 3/3\n",
      "5120/5120 [==============================] - 10s 2ms/step - loss: 1.3813 - acc: 0.6146 - val_loss: 1.1649 - val_acc: 0.6664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2099de3ac18>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE=200\n",
    "crf_model.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=EPOCHS, validation_data=[x_val,y_val])\n",
    "#crf_model.save('../ckpt/crf.h5')\n",
    "#crf_model.save_weights('../ckpt/crf_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.保存模型所有参数\n",
    "modelWeights = crf_model.get_weights()\n",
    "\n",
    "with open('../ckpt/modelWeights.pkl', 'wb') as outp:\n",
    "    pickle.dump(modelWeights, outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Model(inputLength,inputDim,outputDim,class_num):\n",
    "    # define model\n",
    "    inputs = Input(shape=(inputLength,))\n",
    "    x = Embedding(inputDim, outputDim,trainable=False)(inputs)  # mask_zero=True\n",
    "    x = Bidirectional(LSTM(128))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    preds = Dense(class_num, activation='softmax')(x)\n",
    "    model = Model(inputs,preds)\n",
    "    model.compile('adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 45, 256)           1965312   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 34)                8738      \n",
      "=================================================================\n",
      "Total params: 2,368,290\n",
      "Trainable params: 402,978\n",
      "Non-trainable params: 1,965,312\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "fx_model = init_Model(45,7677,256,34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型参数并配置\n",
    "with open('../ckpt/modelWeights.pkl', 'rb') as inp:\n",
    "    modelWeights = pickle.load(inp)\n",
    "\n",
    "\n",
    "fx_model.set_weights(modelWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 1s 993us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1649398617446423, 0.6664062514901161]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_model.evaluate(x_val,y_val,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense, Dropout,Flatten,Conv1D,MaxPooling1D\n",
    "\n",
    "def train_cnn_model(x_train, y_train,x_val, y_val, embedding_mat):\n",
    "    inputSize,inputLength = x_train.shape\n",
    "    print('inputLength=%s'%inputLength)\n",
    "    inputDim,outputDim = embedding_mat.shape\n",
    "    print('inputDim=%s,outputDim=%s'%(inputDim,outputDim))\n",
    "    _,class_num = y_train.shape\n",
    "    print('class_num=%s'%class_num)\n",
    "    \n",
    "    inputs = Input(shape=(inputLength,))\n",
    "    x = Embedding(inputDim,outputDim,weights=[embedding_mat], trainable=False)(inputs)\n",
    "    \n",
    "    x = Conv1D(32,5,activation='relu')(x)\n",
    "    x = MaxPooling1D()(x)\n",
    "\n",
    "    x = Conv1D(64,5,activation='relu')(x)\n",
    "    x = MaxPooling1D()(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(class_num, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs,outputs)\n",
    "    # 损失函数使用交叉熵\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(x_train,y_train,epochs=Epoch,batch_size=BATCH_SIZE,validation_data=(x_val,y_val))\n",
    "    model.save('../ckpt/cnn.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputLength=45\n",
      "inputDim=7677,outputDim=256\n",
      "class_num=34\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 45, 256)           1965312   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 41, 32)            40992     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 20, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 16, 64)            10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 34)                4386      \n",
      "=================================================================\n",
      "Total params: 2,086,658\n",
      "Trainable params: 121,346\n",
      "Non-trainable params: 1,965,312\n",
      "_________________________________________________________________\n",
      "Train on 5120 samples, validate on 1280 samples\n",
      "Epoch 1/1\n",
      "5120/5120 [==============================] - 3s 542us/step - loss: 1.9430 - acc: 0.4613 - val_loss: 1.7095 - val_acc: 0.5492\n"
     ]
    }
   ],
   "source": [
    "cn_model = train_cnn_model(x_train,y_train,x_val, y_val,embedding_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense, Dropout,Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "def train_cnn_model(x_train, y_train,x_val, y_val, embedding_mat):\n",
    "    \n",
    "    x_train = [[embedding_mat[w] for w in s] for s in x_train]\n",
    "    x_train = np.array(x_train, dtype=np.float32)\n",
    "    \n",
    "    _,vocab_size,word2vec_size = x_train.shape\n",
    "    x_train = x_train.reshape(-1,vocab_size,word2vec_size,1)\n",
    "    _,class_num = y_train.shape\n",
    "    \n",
    "    x_val = [[embedding_mat[w] for w in s] for s in x_val]\n",
    "    x_val = np.array(x_val, dtype=np.float32)\n",
    "    x_val = x_val.reshape(-1, vocab_size, word2vec_size, 1)\n",
    "    \n",
    "    inputs = Input(shape=(vocab_size,word2vec_size,1))   #width,height,channels\n",
    "\n",
    "    x = Conv2D(32,(5,5),activation='relu')(inputs)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Conv2D(64,(5,5),activation='relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    outputs = Dense(class_num, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs,outputs)\n",
    "    # 损失函数使用交叉熵\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(x_train,y_train,epochs=Epoch,batch_size=BATCH_SIZE,validation_data=(x_val,y_val))\n",
    "    model.save('../ckpt/cnn.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
